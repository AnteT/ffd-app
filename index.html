<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="assets/css/styles.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FFD Project</title>
  </head>
  <body class="holy-grail">
    <header class="main-header">
      <div>
        <object type="image/svg+xml" data="assets/figs/uwlogo.svg" id="main-svg" style="flex:1;max-width: fit-content;">UWLogo</object>
      </div>
      <div style="padding:0px 0px;padding-right:10px;" style="flex: 6;">
        Artificial Facial Detection & Classification<br>
        <span style="font-size: 11pt;">Separating Real Faces from the Artificially Generated</span><br>
      </div>
      <div>
        &emsp;
        <!-- Empty placeholder for flex box -->
      </div>
    </header>
    <!-- Subheader details -->
    <div class="subheader">
      UW Madison - CS 766 - Ante Tonkovic-Capin & Udhbhav Gupta
    </div>
    <div class="holy-grail-body">
      <section class="holy-grail-content">
          <!-- Introduction Placeholder -->
              <h1 class="std-header">Introduction</h1>
              <hr>
              <p>
                This webpage is part of a submission for Computer Vision 766 at the University of Wisconsin-Madison School of Computer Sciences. The project details the research and findings related to identification and classification of artificially generated human faces. Authored by Ante Tonkovic-Capin and Udhbhav Gupta during the Spring 2024 semester.
              </p>

          <!-- Table of Contents -->
              <h1 class="std-header" id="motivation">Table of Contents</h1>
              <ul style="text-align: left;">
                  <li><a href="#demonstration">Demonstration</a></li>
                  <li><a href="https://github.com/AnteT/ffd-app/tree/master/presentation-slides">Presentation Slides</a></li>
                  <li><a href="https://github.com/AnteT/cs766-project">Project Repository</a></li>
                  <li><a href="#motivation">Motivation</a></li>
                  <li><a href="#real-vs-fake">Defining Real</a></li>
                  <li><a href="#related-work">Related Work</a></li>
                  <li><a href="#methodology">Methodology</a></li>
                  <li><a href="#conclusion">Conclusion</a></li>
              </ul>    

              <!-- demonstration -->
              <h1 class="std-header" id="demonstration">Model Demonstration</h1>
              <hr>
              <p>
                You can find our pretrained model, along with a ready-made inference script that takes an image path as input and returns the result of inference as well as displaying the result of the facial feature extraction, <a href="https://github.com/AnteT/ffd-app/tree/master/inference">here</a>. Below is the result of running inference on two example images, one of a real human face and one of an artificially generated one:
              </p>     
              <figure>
                <div style="display: flex;">
                  <img src="assets/figs/output-real.png" alt="Demo: Real Face" class="std-image std-green" style="width:42vw;margin-right:10px;">
                  <img src="assets/figs/output-fake.png" alt="Demo: Fake Face" class="std-image std-red" style="width:42vw;">
                </div>
                <figcaption style="margin-top: 6px;">
                  Test results for Fake Facial Detection with Facial Extraction, Real sample (left) and Fake sample (right) using inference demo.
                </figcaption>
              </figure>     
              <p>
                To reproduce the baseline examples above, or to run inference on your own images, clone or download the contents of the <span class="std-code">./inference</span> directory available <a href="https://github.com/AnteT/ffd-app">here</a> 
                then install the requirements with <span class="std-code">pip install -r requirements.txt</span>. From there you can run the above examples directly with the provided sample images using <span class="std-code">run_inference.py</span> or provide your own images for inference with <span class="std-code">$ python run_inference.py "image.png"</span> to see the results!
              </p>

              <!-- Motivation -->
              <h1 class="std-header" id="motivation">Motivation</h1>
              <hr>
              <p>
                Artificially generated image technology has become increasingly sophisticated, sometimes termed as 'Deep Fakes', allowing the creation of realistic videos and images that can deceive viewers. In recent years this has become even more acute, with increasingly advanced techniques for creating artificial images and detecting them leading to a sort of arms race in the field. This is particularly dangerous when it comes to false or misleading representations of individuals. Detecting fake or manipulated images in the case where individuals are artificially generated is crucial to prevent their malicious use in spreading misinformation, identity theft, and other harmful activities. Our project aims to explore the potential of implementing a AI generated detection system for facial images using modern computer vision techniques.                
              </p>
              <p>
                Below is an example of just how powerful modern image generation and manipulation techniques have become. Using the original Mona Lisa, just a single image as reference, multiple different perspectives and poses can be generated and seemingly bring her to life:
              </p>
              <figure>
                <img src="https://s2.glbimg.com/5dLkOHpd5r0KZ0S3Pr1BtgP5krU=/e.glbimg.com/og/ed/f/original/2019/05/24/monalisa.gif" alt="Monalisa Comes to Life GIF" style="width: 70vw" class="std-shadow">
                <figcaption>
                  Egor Zakharov Skolkovo Institute of Science
                </figcaption>
              </figure>
              <!-- Defining Real vs Fake -->
              <h1 class="std-header" id="real-vs-fake">Defining Real vs Fake</h1>
              <hr>
              <p>Before moving on, we first need to establish what we mean when we say a face is either "Real" or "Fake". This is not as easy as it sounds, especially when trying to establish an objective line between standard processing and false represntation. 
                
                With the widespread use of image processing, filters on social media, image manipulation has become a widespread phenomenon. 
                These tools have democratized the ability to alter images, making it possible for anyone to modify their photos with just a few taps on a screen. 
                From simple filters that adjust lighting and color, to more complex features that can smooth skin, change eye color, or even reshape facial features, these apps have transformed the way we present ourselves online. 
                While these tools can be used creatively and for fun, they also raise questions about authenticity and blur the line between what should be considered “real” and “fake”.
                Take these four images as an example, as we go from left to right we increasingly manipulate the image using various techniques:
              </p>
              <figure>
                <img src="assets/figs/rvf1.png" alt="RVF: Original" class="std-image" style="width:19vw;margin-right:10px;">
                <img src="assets/figs/rvf2.png" alt="RVF: Second" class="std-image" style="width:19vw;margin-right:10px;">
                <img src="assets/figs/rvf3.png" alt="RVF: Third" class="std-image" style="width:19vw;margin-right:10px;">
                <img src="assets/figs/rvf4.png" alt="RVF: Fourth" class="std-image" style="width:19vw;">
                <figcaption style="margin-top: 6px;">
                  Right image generated using Stable Diffusion model at https://www.artguru.ai/ai-text-to-image-generator using left-most image as input
                </figcaption>
              </figure>              
              <p>
                The left-most image is the unaltered original, even here "unaltered" still includes the native iPhone 14 camera's compensation and processing. 
                The second image has only been modified in changing the original color to black & white. 
                The third image has additional modifications, including some sharp changes to contrast and clipping pixel intensities at both ends of the color range with the addition of a slight Gaussian blur as well. 
                The fourth image is the output from Stable Diffusion as a result of the prompt "modify the faces of the human and the dog" when provided with the original unaltered image. In the case of these four images, you could fairly argue that the first two should be classified as "Real", while the second two considered "Fake". Easy enough right?

                Instead of four discrete stages of modifications, consider we have a spectrum of all the images inbetween these four. Each step slightly more manipulated and modified than the last. At which image, which level of alteration from the original image, can we stop and draw a line to declare everything beyond this level is considered "Fake"? Considering this difficulty, we used the following verbose but careful definition when defining what constitues a "Real" human facial image:
                </p>
                <p style="font-style: italic;padding-left:30px;text-indent: 0px;">
                  The face, as digitally represented, is considered real if it existed physically as a biological human at a moment in time and faithfully depicts the countenance, demeanor and position originally expressed by the digital representation such that the average observer's prima facie impression would reflect the reality of the moment captured.
                </p>
                <p>
                All this to say that we consider an image of a face as "Real" if it accurately represents a moment in time, and the average observer should be left with an accurate impression of the moment when presented the image that reflects it. For the purposes of this project, it is easier to classify what constitues a "Fake" image of a face, the majority we use are those artificially generated by GAN models or models performing localized manipulations of facial features.
              </p>
              <!-- Related Work -->
              <h1 class="std-header" id="related-work">Related Work</h1>
              <hr>
              <p>
                Related Work placeholder
              </p>
              <!-- Methodology -->
              <h1 class="std-header" id="methodology">Methodology</h1>
              <hr>
              <p>
                At a high level, our project takes a two-phased approach. The first phase leverages <a href="https://github.com/ipazc/mtcnn">Multitask Cascaded Convolutional Networks</a> or MTCNN.
                MTCNN is a powerful algorithm for facial detection, iteratively feeding candidate detections through 3 different networks:
              </p>              
                <ul style="text-align: left;">
                    <li><b>P-Net</b>: Proposal network, used to generate candidate bounding boxes around candidate detections</li>
                    <li><b>R-Net</b>: Refine network, refines and eliminates false positive bounding boxes, predicts facial landmarks like eyes, nose, mouth</li>
                    <li><b>O-Net</b>: Output network, performs final calibration on bounding boxes, refines landmark predictions.</li>
                </ul>
              <p style="text-indent: 0px;">
                These combined networks allow efficient and highly accurate detections, here's an example of MTCNN in action:
              </p>
              <figure>
                <img src="assets/figs/mtcnn-detection.png" alt="MTCNN: Example" class="std-image" style="width:58vw;">
                <figcaption style="margin-top: 6px;">
                  MTCNN detecting and isolating two candidates, one human and one canine
                </figcaption>
              </figure>   
              <p>
                Once the face is detected and isolated using the MTCNN phase, we extract it from the image and apply any required resizing and normalization. 
                Here you can see that, while MTCNN is capable of detecting the canine's face, we only grab the first most likely facial candidate to use in our pipeline:
              </p>
              <figure>
                <img src="assets/figs/mtcnn-extract.png" alt="MTCNN: Example" style="width:58vw;">
                <figcaption style="margin-top: 6px;">
                  MTCNN detection and extraction between candidates, one human and one canine
                </figcaption>
              </figure>   
                <p>The second phase of our process involves a CNN classifier trained on the output of the first phase, the extracted facial images output from the MTCNN model. 
                  We tried multiple different architectures and approaches to find the best CNN when it came to classifying the images as either "Real" or "Fake". 
                  Ultimately, we found a four layer model, two convolutional layers and two fully connected ones, provided the best results. Here's an ONNX representation of the model we used:
                </p>
                <figure>
                  <img src="assets/figs/cnn-graph-crop.png" alt="CNN: ONNX" style="width:80vw; border:2px solid #AEBD57;" class="std-image">
                  <figcaption style="margin-top: 6px;">
                    Fake Facial Detection CNN Classifier ONNX Graph
                  </figcaption>
                </figure> 
                <p>
                  We weren't initially sure if our two-phased approach would indeed produce better results than simply training our CNN against the full image and without any facial extraction using MTCNN. 
                  This is because we achieved better test loss and higher test accuracy when using only our trained CNN classifier versus using both. 
                  Here's the training results from both these approaches, using only the Fake Facial Detector CNN (FDD) versus using our two-phased approach of first detection and extraction through the MTCNN model, followed by a second phase with our CNN classifier (FFX+FFD), using the entire 140,000 image dataset across 10 full training epochs:
                </p>
                <figure>
                  <img src="assets/figs/ffd-vs-ffx-loss.png" alt="FFD vs FFX: Loss" class="std-image std-plum" style="width:40vw;margin-right:10px;">
                  <img src="assets/figs/ffd-vs-ffx-accuracy.png" alt="FFD vs FFX: Accuracy" class="std-image std-plum" style="width:40vw;">
                  <figcaption style="margin-top: 6px;">
                    Test results for Fake Facial Detection with and without Facial Extraction, loss (left) and test accuracy (right) across epochs.
                  </figcaption>
                </figure>     

              <!-- Conclusion -->
              <h1 class="std-header" id="conclusion">Conclusion</h1>
              <hr>
              <p>
                Conclusion placeholder
              </p>

            <div style="padding-bottom:40px;">
            <!-- Extra padding before footer -->
            </div>
            </section>
            <div class="holy-grail-sidebar-left hg-sidebar">
              <!-- left sidebar -->
            </div>
            <div class="holy-grail-sidebar-right hg-sidebar">
              <!-- right sidebar -->
            </div> 
          </div>
          <footer class="main-footer">
            Created for CS 766, University of Wisconsin-Madison Graduate School of Computer Sciences
          </footer>
      </body>
</html>