<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="assets/css/styles.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FFD Project</title>
  </head>
  <body class="holy-grail">
    <header class="main-header">
      <div>
        <object type="image/svg+xml" data="assets/figs/uwlogo.svg" id="main-svg" style="flex:1;max-width: fit-content;">UWLogo</object>
      </div>
      <div style="padding:0px 0px;padding-right:10px;" style="flex: 6;">
        Artificial Facial Detection & Classification<br>
        <span style="font-size: 11pt;">Separating Real Faces from the Artificially Generated</span><br>
      </div>
      <div>
        &emsp;
        <!-- Empty placeholder for flex box -->
      </div>
    </header>
    <!-- Subheader details -->
    <div class="subheader">
      UW Madison - CS 766 - Ante Tonkovic-Capin & Udhbhav Gupta
    </div>
    <div class="holy-grail-body">
      <section class="holy-grail-content">
          <!-- Introduction Placeholder -->
              <h1 class="std-header">Introduction</h1>
              <hr>
              <p>
                This webpage is part of a submission for Computer Vision 766 at the University of Wisconsin-Madison School of Computer Sciences. The project details the research and findings related to identification and classification of artificially generated human faces. Authored by Ante Tonkovic-Capin and Udhbhav Gupta during the Spring 2024 semester.
              </p>

          <!-- Table of Contents -->
              <h1 class="std-header" id="motivation">Table of Contents</h1>
              <ul style="text-align: left;">
                  <li><a href="#demonstration">Demonstration</a></li>
                  <li><a href="https://github.com/AnteT/ffd-app/tree/master/presentation-slides">Presentation Slides</a></li>
                  <li><a href="https://github.com/AnteT/cs766-project">Project Repository</a></li>
                  <li><a href="#motivation">Motivation</a></li>
                  <li><a href="#real-vs-fake">Defining Real</a></li>
                  <li><a href="#related-work">Related Work</a></li>
                  <li><a href="#methodology">Methodology</a></li>
                  <li><a href="#results">Results</a></li>
                  <li><a href="#conclusion">Conclusion</a></li>
              </ul>    

              <!-- demonstration -->
              <h1 class="std-header" id="demonstration">Model Demonstration</h1>
              <hr>
              <p>
                You can find our pretrained model, along with a ready-made inference script that takes an image path as input and returns the result of inference as well as displaying the result of the facial feature extraction, <a href="https://github.com/AnteT/ffd-app/tree/master/inference">here</a>. Below is the result of running inference on two example images, one of a real human face and one of an artificially generated one:
              </p>     
              <figure>
                <div style="display: flex; justify-content: center">
                  <img src="assets/figs/output-real.png" alt="Demo: Real Face" class="std-image std-green" style="width:35vw;margin-right:10px;">
                  <img src="assets/figs/output-fake.png" alt="Demo: Fake Face" class="std-image std-red" style="width:35vw;">
                </div>
                <figcaption style="margin-top: 6px;">
                  Test results for Fake Facial Detection with Facial Extraction, Real sample (left) and Fake sample (right) using inference demo.
                </figcaption>
              </figure>     
              <p>
                To reproduce the baseline examples above, or to run inference on your own images, clone or download the contents of the <span class="std-code">./inference</span> directory available <a href="https://github.com/AnteT/ffd-app">here</a> 
                then install the requirements with <span class="std-code">pip install -r requirements.txt</span>. From there you can run the above examples directly with the provided sample images using <span class="std-code">run_inference.py</span> or provide your own images for inference with <span class="std-code">$ python run_inference.py "image.png"</span> to see the results!
              </p>

              <!-- Motivation -->
              <h1 class="std-header" id="motivation">Motivation</h1>
              <hr>
              <p>
                Artificially generated image technology has become increasingly sophisticated, sometimes termed as 'Deep Fakes', allowing the creation of realistic videos and images that can deceive viewers. In recent years this has become even more acute, with increasingly advanced techniques for creating artificial images and detecting them leading to a sort of arms race in the field. This is particularly dangerous when it comes to false or misleading representations of individuals. Detecting fake or manipulated images in the case where individuals are artificially generated is crucial to prevent their malicious use in spreading misinformation, identity theft, and other harmful activities. Our project aims to explore the potential of implementing a AI generated detection system for facial images using modern computer vision techniques.                
              </p>
              <p>
                Below is an example of just how powerful modern image generation and manipulation techniques have become. Using the original Mona Lisa, just a single image as reference, multiple different perspectives and poses can be generated and seemingly bring her to life:
              </p>
              <figure>
                <img src="https://s2.glbimg.com/5dLkOHpd5r0KZ0S3Pr1BtgP5krU=/e.glbimg.com/og/ed/f/original/2019/05/24/monalisa.gif" alt="Monalisa Comes to Life GIF" style="width: 55vw" class="std-shadow">
                <figcaption>
                  Egor Zakharov Skolkovo Institute of Science
                </figcaption>
              </figure>

              <!-- Defining Real vs Fake -->
              <h1 class="std-header" id="real-vs-fake">Defining Real vs Fake</h1>
              <hr>
              <p>Before moving on, we first need to establish what we mean when we say a face is either "Real" or "Fake". This is not as easy as it sounds, especially when trying to establish an objective line between standard processing and false represntation. 
                
                With the widespread use of image processing, filters on social media, image manipulation has become a widespread phenomenon. 
                These tools have democratized the ability to alter images, making it possible for anyone to modify their photos with just a few taps on a screen. 
                From simple filters that adjust lighting and color, to more complex features that can smooth skin, change eye color, or even reshape facial features, these apps have transformed the way we present ourselves online. 
                While these tools can be used creatively and for fun, they also raise questions about authenticity and blur the line between what should be considered “real” and “fake”.
                Take these four images as an example, as we go from left to right we increasingly manipulate the image using various techniques:
              </p>
              <figure>
                <img src="assets/figs/rvf1.png" alt="RVF: Original" class="std-image" style="width:17vw;margin-right:10px;">
                <img src="assets/figs/rvf2.png" alt="RVF: Second" class="std-image" style="width:17vw;margin-right:10px;">
                <img src="assets/figs/rvf3.png" alt="RVF: Third" class="std-image" style="width:17vw;margin-right:10px;">
                <img src="assets/figs/rvf4.png" alt="RVF: Fourth" class="std-image" style="width:17vw;">
                <figcaption style="margin-top: 6px;">
                  Right image generated using Stable Diffusion model at https://www.artguru.ai/ai-text-to-image-generator using left-most image as input
                </figcaption>
              </figure>              
              <p>
                The left-most image is the unaltered original, even here "unaltered" still includes the native iPhone 14 camera's compensation and processing. 
                The second image has only been modified in changing the original color to black & white. 
                The third image has additional modifications, including some sharp changes to contrast and clipping pixel intensities at both ends of the color range with the addition of a slight Gaussian blur as well. 
                The fourth image is the output from Stable Diffusion as a result of the prompt "modify the faces of the human and the dog" when provided with the original unaltered image. In the case of these four images, you could fairly argue that the first two should be classified as "Real", while the second two considered "Fake". Easy enough right?

                Instead of four discrete stages of modifications, consider we have a spectrum of all the images inbetween these four. Each step slightly more manipulated and modified than the last. At which image, which level of alteration from the original image, can we stop and draw a line to declare everything beyond this level is considered "Fake"? Considering this difficulty, we used the following verbose but careful definition when defining what constitues a "Real" human facial image:
                </p>
                <p style="font-style: italic;padding-left:30px;text-indent: 0px;">
                  The face, as digitally represented, is considered real if it existed physically as a biological human at a moment in time and faithfully depicts the countenance, demeanor and position originally expressed by the digital representation such that the average observer's prima facie impression would reflect the reality of the moment captured.
                </p>
                <p>
                All this to say that we consider an image of a face as "Real" if it accurately represents a moment in time, and the average observer should be left with an accurate impression of the moment when presented the image that reflects it. For the purposes of this project, it is easier to classify what constitues a "Fake" image of a face, the majority we use are those artificially generated by GAN models or models performing localized manipulations of facial features.
              </p>

              <!-- Related Work -->
              <h1 class="std-header" id="related-work">Related Work</h1>
              <hr>
              <p>
                Detection of fake or manipulated images has been a topic of interest for many years, pre-dating the advent of Machine Learning models. Traditionally digital images were manually manipulated using software like Photoshop.
                Gradually multiple types of Machine Learnign models were developed which could manipulate parts of images for example facial expressions. More recently with the development in Generaive AI, ML models, such as Generative Adversarial Networks (GANs), have become quite capable of generating entire images on their own based on text prompts.
                A variety of approaches and techniques have been developed over time to identify manipulated or AI generated images. Some of these include:
                <ul style="text-align: left;">
                  <li><b>Image Forensics and Metadata Analysis</b>: Techniques that analyze image metadata e.g. EXIF data or the image itself for inconsistencies, artifacts, or other signs of manipulation. 
                    Artifacts include image watermarks, sensor noise patterns, file format compression signals and lighting and lens aberrations. 
                    Such techniques have worked well on Photoshop based forgery but fall short when applied to AI generated images. Further, once such artifacts are exposed modern ML models can then be explicitly trained to work around them.
                  </li>
                  <li><b>Deep Learning</b>: Using (convolutional) neural networks to classify images as real or fake based on a training dataset. Once trained, such models end up resembling the Discriminator of the GAN whose dataset the model is trained on. 
                    While these models work well on this specific dataset, they don't generalize well to images generated from other GANs.
                  </li>
                  <li><a href="https://arxiv.org/abs/1808.07276"><b>Color Statistics feature set</b></a>: To detect GAN generated images, not specifically faces, the authors of this paper propose a feature set based on color statistics. 
                    They argue that GAN generated images have a different color distribution than real images, and that this difference can be used to detect them. Detection models based on this feature set have been shown to generalize well to multiple GANs.
                  </li>
                  <li><a href="https://arxiv.org/abs/1809.08754"><b>DeepFD</b></a>: A deep learning based joint feature learning and classification model has been proposed which uses a combination of CNN and LSTM with contrastive loss to model common underlying feature os fake images that differ from real images. 
                    This model has also been shown to generalize well to multiple GANs.
                  </li>
                </ul>
              </p>

              <!-- Methodology -->
              <h1 class="std-header" id="methodology">Methodology</h1>
              <hr>
              <p>
                At a high level, our project takes a two-phased approach. The first phase leverages <a href="https://github.com/ipazc/mtcnn">Multitask Cascaded Convolutional Networks</a> or MTCNN.
                MTCNN is a powerful algorithm for facial detection, iteratively feeding candidate detections through 3 different networks:
              </p>              
                <ul style="text-align: left;">
                    <li><b>P-Net</b>: Proposal network, used to generate candidate bounding boxes around candidate detections</li>
                    <li><b>R-Net</b>: Refine network, refines and eliminates false positive bounding boxes, predicts facial landmarks like eyes, nose, mouth</li>
                    <li><b>O-Net</b>: Output network, performs final calibration on bounding boxes, refines landmark predictions.</li>
                </ul>
              <p style="text-indent: 0px;">
                These combined networks allow efficient and highly accurate detections, here's an example of MTCNN in action:
              </p>
              <figure>
                <img src="assets/figs/mtcnn-detection.png" alt="MTCNN: Example" class="std-image" style="width:45vw;">
                <figcaption style="margin-top: 6px;">
                  MTCNN detecting and isolating two candidates, one human and one canine
                </figcaption>
              </figure>   
              <p>
                Once the face is detected and isolated using the MTCNN phase, we extract it from the image and apply any required resizing and normalization. 
                Here you can see that, while MTCNN is capable of detecting the canine's face, we only grab the first most likely facial candidate to use in our pipeline:
              </p>
              <figure>
                <img src="assets/figs/mtcnn-extract.png" alt="MTCNN: Example" style="width:45vw;">
                <figcaption style="margin-top: 6px;">
                  MTCNN detection and extraction between candidates, one human and one canine
                </figcaption>
              </figure>   
                <p>The second phase of our process involves a CNN classifier trained on the output of the first phase, the extracted facial images output from the MTCNN model. 
                  We tried multiple different architectures and approaches to find the best CNN when it came to classifying the images as either "Real" or "Fake". 
                  Ultimately, we found a four layer model, two convolutional layers and two fully connected ones, provided the best results. Here's an ONNX representation of the model we used:
                </p>
                <figure>
                  <img src="assets/figs/cnn-graph-crop.png" alt="CNN: ONNX" style="width:80vw; border:2px solid #AEBD57;" class="std-image">
                  <figcaption style="margin-top: 6px;">
                    Fake Facial Detection CNN Classifier ONNX Graph
                  </figcaption>
                </figure>

                <p>
                  The overall pipeline looks like this:
                </p>
                <figure>
                  <img src="assets/figs/system_pipeline.png" alt="FFD + FFX: Pipeline" class="std-image" style="width:40vw;margin-right:10px;">
                  <figcaption style="margin-top: 6px;">
                    Fake Facial Detection Pipeline
                  </figcaption>
                </figure>
                    
              
              <!-- Results -->
              <h1 class="std-header" id="results">Results</h1>
              <hr>
              <p>
                We initially started started our project by exploring which model type would work best for classifying images as fake or real with our candidates being the Vision Transformer (ViT) and Convolutional Neural Network (CNN).
                Vision Transformers are a relatively new architecture that divides images into patches, converts patches into vector embeddings, and then uses a transformer encoder to learn dependencies and relationships between different patches.
                ViTs typically perform well on image classification tasks, but we found that they did not perform as well as CNNs on our dataset. This can likely be attributed to the less emphasis that ViTs place on local features and perhaps the need for a larger dataset to train on.
                We trained both models on the Photoshop and StyleGAN dataset with different configurations and hyperparameters, and found that the CNN model performed better.
              </p>


              <p>
                Next we trained the CNN with and without the MTCNN facial extraction phase. 
                Here's the training results from both these approaches, using only the Fake Facial Detector CNN (FFD) versus using our two-phased approach of first detection and extraction through the MTCNN model, followed by a second phase with our CNN classifier (FFX+FFD), using the entire 140,000 image dataset across 10 full training epochs:
              </p>
              <figure>
                <img src="assets/figs/ffd-vs-ffx-loss.png" alt="FFD vs FFX: Loss" class="std-image std-plum" style="width:40vw;margin-right:10px;">
                <img src="assets/figs/ffd-vs-ffx-accuracy.png" alt="FFD vs FFX: Accuracy" class="std-image std-plum" style="width:40vw;">
                <figcaption style="margin-top: 6px;">
                  Test results for Fake Facial Detection with and without Facial Extraction, loss (left) and test accuracy (right) across epochs.
                </figcaption>
              </figure>
             
              <p>
                Based on the above results, it would appear that the higher accuracy of the FFD than FFX+FFD indicates that the MTCNN phase is not improving overall classification process.
                However, the higher accuracy of the FFD model can be attributed to the fact that without FX the model has more background pixels to learn from (256x256 vs 160x160 with FX), so the model captures more information about synthetically generated pixels.
                But, those background pixels do not provide any information about the synthetic facial features which we ideally want our model to learn. By using MTCNN to extract the face, we we not only restrict the model to only learn on pixels representing facial features, but we also make the model capture information about different facial orientations better.
                To highlight this point, take a look at the left-most image in Figure 2 below. This is a face generated by StyleGAN which the FFD model classified as real with a high probability (0.9997) but the FFX+FFD model correctly classified as fake.
              </p>

              <p>
                After training the 2 models only on the StyleGAN dataset, we also wanted to test how the model generalizes to other datasets of fake images. Following are the results:
              </p>
              <figure>
                <img src="assets/figs/generalization.png" alt="Model generalization results" class="std-image" style="width:70vw;margin-right:10px;">
                <figcaption style="margin-top: 6px;">
                  Figure 1: Model Accuracy of FFD and FFX+FFD on various datasets
                </figcaption>
              </figure>
              
              <br>

              <figure>
                <img src="assets/figs/img_samples.png" alt="Sample dataset images" class="std-image" style="width:70vw;margin-right:10px;">
                <figcaption style="margin-top: 6px;">
                  Figure 2: Sample dataset images correctly classified with Facial Extraction but not without
                </figcaption>
              </figure>

              <p>
                Unsurprisingly, the overall test accuracy of both models was low on StarGAN and DALL-E which falls in line with the fact that regular CNNs only tend to work well for the GAN whose images they are trained on and don’t generalize well to other GANs.
                However the model did surprsingly well on the Photoshop image dataset, which we didn't expect.
              </p>
              <p>
                As apparent in the results in Figure 1, a common theme we noticed was that, despite being trained only on the StyleGAN dataset, the CNN model with MTCNN Facial Extraction had better accuracy on the other GAN datasets than the model without Facial Extraction.
                <b> So, the main takeaway from these results was that the CNN model with MTCNN Facial Extraction was able to generalize better to other GANs, than the model without Facial Extraction.</b>
              </p>

              <!-- Conclusion -->
              <h1 class="std-header" id="conclusion">Conclusion</h1>
              <hr>
              <ul style="text-align: left;">
                <li>Our project is a step towards detecting fake or manipulated images in the case where individuals are artificially generated, which is crucial to prevent their malicious use in spreading misinformation, identity theft, and other harmful activities.</li>
                <li>MTCNN is one of the most accurate facial detection models. For inference tasks, it is fast, accurate and also identifies facial landmarks. </li>
                <li>While the model we developed only produced accurate detection on the dataset it was trained on, we found that pre-processing image data with MTCNN for facial extraction provides more reliable results from the CNN classifier and tends to generalizes better. 
                <li>Classifying images as real or AI generated remains a challenge and active area of research, particularly from a generalization perspective. This project shows that Facial Extraction can improve the generalization of detection models.
                </li>                
              </ul>

            <div style="padding-bottom:40px;">
            <!-- Extra padding before footer -->
            </div>
            </section>
            <div class="holy-grail-sidebar-left hg-sidebar">
              <!-- left sidebar -->
            </div>
            <div class="holy-grail-sidebar-right hg-sidebar">
              <!-- right sidebar -->
            </div> 
          </div>
          <footer class="main-footer">
            Created for CS 766, University of Wisconsin-Madison Graduate School of Computer Sciences
          </footer>
      </body>
</html>
